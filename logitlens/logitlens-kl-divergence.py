# -*- coding: utf-8 -*-
"""Logitlens KL Divergence.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qNEKd3AU4NmbKVhe79DinBOdbOg63GUp
"""

import torch
import torch.nn.functional as F
import matplotlib.pyplot as plt
import numpy as np
from transformers import AutoModelForCausalLM, AutoTokenizer
from datasets import load_dataset
import argparse

def main():
    parser = argparse.ArgumentParser(description="Logitlens KL Divergence")
    parser.add_argument("--model_name", type=str, default="Qwen/Qwen2.5-Math-7B", help="Model name from Hugging Face")
    parser.add_argument("--n_problems", type=int, default=5, help="Number of problems to process")
    parser.add_argument("--max_length", type=int, default=256, help="Max length for tokenizer")
    parser.add_argument("--output_file", type=str, default="kl_divergence_plot.png", help="Output file for the plot")
    args = parser.parse_args()

    N_PROBLEMS = args.n_problems
    MODEL_NAME = args.model_name
    DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
    DTYPE = torch.float16
    MAX_LENGTH = args.max_length

    ds = load_dataset("HuggingFaceH4/MATH-500", split="test[:{}]".format(N_PROBLEMS))
    problems = [f"Q: {item['problem']} A:" for item in ds]

    model = AutoModelForCausalLM.from_pretrained(
        MODEL_NAME, torch_dtype=DTYPE, device_map=None, low_cpu_mem_usage=True
    ).to(DEVICE)
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
    model.eval()

    # --- Single-prompt KL computation ---
    def kl_per_prompt(text):
        layer_logits = []

        def make_hook():
            def hook(module, inputs, outputs):
                hidden = outputs[0] if isinstance(outputs, tuple) else outputs
                hidden = model.model.norm(hidden)
                logits = hidden @ model.lm_head.weight.T
                layer_logits.append(logits.detach().cpu())
            return hook

        hooks = [blk.register_forward_hook(make_hook()) for blk in model.model.layers]

        inputs = tokenizer(
            text, return_tensors="pt", truncation=True, max_length=MAX_LENGTH
        ).to(DEVICE)

        with torch.no_grad():
            final_logits = model(**inputs).logits.detach().cpu()

        for h in hooks:
            h.remove()

        kl_scores = []
        for logits in layer_logits:
            q = F.log_softmax(logits[:, -1, :], dim=-1)
            p = F.softmax(final_logits[:, -1, :], dim=-1)
            kl = F.kl_div(q, p, reduction="batchmean")
            kl_scores.append(kl.item())

        return kl_scores

    all_curves = []
    for i, prompt in enumerate(problems):
        print(f"Processing problem {i+1}/{N_PROBLEMS}...")
        kl_curve = kl_per_prompt(prompt)
        all_curves.append(kl_curve)

    max_len = max(len(c) for c in all_curves)
    arr = np.array([
        np.pad(c, (0, max_len - len(c)), constant_values=np.nan)
        for c in all_curves
    ])
    mean_curve = np.nanmean(arr, axis=0)
    std_curve = np.nanstd(arr, axis=0)

    plt.figure(figsize=(8, 4))
    x = np.arange(len(mean_curve), step=1)
    plt.plot(x, mean_curve, color="blue", label="Mean KL-Divergence")
    plt.fill_between(
        x, mean_curve - std_curve, mean_curve + std_curve,
        color="blue", alpha=0.2, label="±1 Std"
    )
    plt.title(f"{MODEL_NAME} – Average LogitLens KL vs Layer ({N_PROBLEMS} MATH-500 Problems)")
    plt.xlabel("Layer index")
    plt.ylabel("Average KL Divergence to Final Logits")
    plt.grid(True)
    plt.legend()
    plt.xticks(x)
    plt.tight_layout()
    plt.savefig(args.output_file)
    print(f"Plot saved to {args.output_file}")

if __name__ == "__main__":
    main()
